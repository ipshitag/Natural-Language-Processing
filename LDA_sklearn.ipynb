{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#libraries required\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS.add('tyre')\n",
    "STOP_WORDS.add(' tyre')\n",
    "STOP_WORDS.add('tyre ')\n",
    "STOP_WORDS.add(' tyre ')\n",
    "STOP_WORDS.add('great')\n",
    "STOP_WORDS.add('good')\n",
    "STOP_WORDS.add('easy')\n",
    "STOP_WORDS.add('excellent')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#model\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#loading small corpus\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#reading data\n",
    "df = pd.read_csv(\"sentisum-assessment-dataset.csv\")\n",
    "#some cleaning\n",
    "df = df.drop(['Unnamed: 1'],axis=1)\n",
    "#renaming\n",
    "df.columns = [\"document\"]\n",
    "\n",
    "#list of contractions and their related expansions (from web)\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\" u \": \" you \",\n",
    "\" ur \": \" your \",\n",
    "\" n \": \" and \",\n",
    "\"tbh\":\"to be honest\" }\n",
    "\n",
    "#functions\n",
    "\n",
    "def get_avg_word_len(x):\n",
    "    \"\"\"Get the average word length from a given sentence\n",
    "    param x(str): the sentence of whose word length is to be taken\n",
    "    return leng(numeric): the average word length \"\"\"\n",
    "\n",
    "    words = x.split()\n",
    "    word_len = 0\n",
    "    for word in words:\n",
    "        word_len = word_len + len(word)\n",
    "    return word_len/len(words)\n",
    "\n",
    "def feature_extract(df,d):\n",
    "    \"\"\"Adds new columns in the given df, from the existing data\n",
    "    count: number of words in the document (df[d])\n",
    "    char count: number of characters in df[d]\n",
    "    avg word_len: the average number of characters in the df[d]\n",
    "    stop_words_len: number of stopwords present\n",
    "    numeric_count: number of numeric characters present\n",
    "    upper_counts: number of words in CAPS LOCK\n",
    "    \n",
    "    param df(dataframe): dataframe on which manipulation is to be done\n",
    "    param d(str): column name in which the reuired words are present\"\"\"\n",
    "    \n",
    "    df['count']=df[d].apply(lambda x: len(str(x).split()))\n",
    "    df['char count']=df[d].apply(lambda x: len(x))\n",
    "    df['avg word_len'] = df[d].apply(lambda x:get_avg_word_len(x))\n",
    "    df['stop_words_len'] = df[d].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
    "    df['numeric_count'] = df[d].apply(lambda x:len([t for t in x.split()if t.isdigit()] ))\n",
    "    df['upper_counts'] = df[d].apply(lambda x: len([t for t in x.split() if t.isupper() and len(x)>3]))\n",
    "\n",
    "\n",
    "def expand(x):\n",
    "    \"\"\"Some of the words like 'i'll', are expanded to 'i will' for better text processing\n",
    "    The list of contractions is taken from the internet\n",
    "    \n",
    "    param x(str): the sentence in which contractions are to be found and expansions are to be done\n",
    "    \n",
    "    return x(str): the expanded sentence\"\"\"\n",
    "    if type(x)== str:\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key,value)\n",
    "        return x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def remove_accented_chars(x):\n",
    "    \"\"\"The function changes the accented characters into their equivalent normal form,\n",
    "    to do so, normalize function with 'NFKD' is used which replaces the compatibility characters into\n",
    "    theri euivalent\n",
    "    \n",
    "    param x(str): the sentence in which accented characters are to be detected and removes\n",
    "    return x(str): sentence with accented characters replaced by their equivalent\"\"\"\n",
    "    \n",
    "    x = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_to_base(x):\n",
    "    \"\"\"Converting the words to their base word and dictionary head word i.e to lemmatize\n",
    "    param x(str): the sentence in which the words are to be converted (lemmatization)\n",
    "    return x(str): the lemmatized sentence\"\"\"\n",
    "    \n",
    "    x_list = []\n",
    "    doc = nlp(x)\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma = str(token.lemma_)\n",
    "        if lemma == '-PRON-' or lemma == 'be':\n",
    "            lemma = token.text\n",
    "        x_list.append(lemma)\n",
    "    return (\" \".join(x_list))\n",
    "    \n",
    "def preprocess(df,d):\n",
    "    \"\"\"Preprocesses the given document by applying the following functionalities\n",
    "    lower: lowers all the characters for uniformity\n",
    "    expansion: expands words like i'll to i will for better text classification\n",
    "    remove special characters: using regex, removes all the punctuations etc\n",
    "    remove space: removes trailing spaces and extra spaces between words\n",
    "    remove accented characters: change accented characters to its normal equivalent\n",
    "    remove stop words: removes the stop words in the sentence\n",
    "    lemmatization: changes the words to their base form\"\"\"\n",
    "    \n",
    "    df[d] = df[d].apply(lambda x: x.lower())\n",
    "    df[d] = df[d].apply(expand)\n",
    "    df[d] = df[d].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n",
    "    df[d] = df[d].apply(lambda x: \" \".join(x.split()))\n",
    "    df[d] = df[d].apply(lambda x: remove_accented_chars(x))\n",
    "    df[d] = df[d].apply(lambda x: make_to_base(x))\n",
    "    df[d] = df[d].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))\n",
    "\n",
    "def get_bow(df,d):\n",
    "    cv = CountVectorizer(ngram_range=(3,4),min_df=5, max_df=0.5)\n",
    "    text_counts = cv.fit_transform(df[d])\n",
    "    bow = pd.DataFrame(text_counts.toarray(), columns = cv.get_feature_names())\n",
    "    return cv,bow\n",
    "\n",
    "def tf_bow(df,d):\n",
    "    tf = TfidfVectorizer(ngram_range=(3,4),min_df=5, max_df=0.5)\n",
    "    text_counts = tf.fit_transform(df[d])\n",
    "    tf_df = pd.DataFrame(text_counts.toarray(), columns = tf.get_feature_names())\n",
    "    return tf,tf_df\n",
    "\n",
    "feature_extract(df,'document')\n",
    "preprocess(df,'document')\n",
    "cvectorizer,data_cv = get_bow(df,'document')\n",
    "tvectorizer,data_tf = tf_bow(df,'document')\n",
    "\n",
    "# Use LDA to look for 12 topics\n",
    "n_topics = 12\n",
    "model_lda = LatentDirichletAllocation(n_components=n_topics,random_state=0)\n",
    "model_lda.fit(data_cv)\n",
    "\n",
    "# Print the top 10 words per topic\n",
    "#n_words = 10\n",
    "#feature_names = cvectorizer.get_feature_names()\n",
    "\n",
    "#topic_list = []\n",
    "#for topic_idx, topic in enumerate(model_lda.components_):\n",
    "#    top_n = [feature_names[i]\n",
    "#             for i in topic.argsort()\n",
    "#                 [-n_words:]][::-1]\n",
    "    \n",
    "#    top_features = ' '.join(top_n)\n",
    "    \n",
    "#    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
    "\n",
    "#    print(f\"Topic {topic_idx}: {top_features}\")\n",
    "#    print('\\n\\n\\n')\n",
    "                      \n",
    "\n",
    "topic_result = model_lda.transform(data_cv)\n",
    "df['topic']  = topic_result.argmax(axis=1)\n",
    "                      \n",
    "#fig = plt.figure(figsize=(80,32))\n",
    "#for i in range(12):\n",
    "#    ax = fig.add_subplot(4,3,i+1)\n",
    "#    topic = i\n",
    "#    text = ' '.join(df.loc[df['topic']==i,'document'].values)\n",
    "#    wc = WordCloud(width=1000, height=1000, random_state=1, background_color='Black',colormap='Set2',collocations=False).generate(text)\n",
    "#    ax.imshow(wc)\n",
    "#    ax.set_title(topic)\n",
    "#    ax.axis(\"off\");\n",
    "                      \n",
    "def assign_topic(x):\n",
    "    if x==0:\n",
    "        return \"ease of booking\"\n",
    "    elif x==1:\n",
    "        return \"value for money\"\n",
    "    elif x==2:\n",
    "        return \"garage service\"\n",
    "    elif x==3:\n",
    "        return \"length of fitting\"\n",
    "    elif x==4:\n",
    "        return \"mobile fitter\"\n",
    "    elif x==5:\n",
    "        return \"tyre quality\"\n",
    "    elif x==6:\n",
    "        return \"delivery punctuality\"\n",
    "    elif x==7:\n",
    "        return \"booking confusion\"\n",
    "    elif x==8:\n",
    "        return \"location\"\n",
    "    elif x==9:\n",
    "        return \"wait time\"\n",
    "    elif x==10:\n",
    "        return \"change of date\"\n",
    "    elif x==11:\n",
    "        return \"discounts\"\n",
    "    \n",
    "df['topic'] = df['topic'].apply(lambda x:assign_topic(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
